# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16CqFN0D1puSAjSDGqzMD8w3oNaZxpqmm
"""

# @title 1. C√†i ƒë·∫∑t & T·∫£i d·ªØ li·ªáu
# --- CELL 1: SETUP ---
import pandas as pd
import networkx as nx
import numpy as np
import random
import torch
import torch.nn as nn
import pickle
import os
import warnings
from scipy import sparse
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_recall_curve
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity
from networkx.algorithms.community import greedy_modularity_communities
from google.colab import files

warnings.filterwarnings('ignore')

# Upload file n·∫øu ch∆∞a c√≥
required_files = ['/content/author_nodes_csLG_v1_2024_2025.csv', '/content/temporal_edges_csLG_v1_2024_2025.csv']
missing = [f for f in required_files if not os.path.exists(f)]
if missing:
    print(f"‚ö†Ô∏è Thi·∫øu file: {missing}. Vui l√≤ng upload:")
    uploaded = files.upload()
else:
    print("‚úÖ ƒê√£ c√≥ ƒë·ªß d·ªØ li·ªáu.")

# @title 2. X·ª≠ l√Ω Time-series
# --- CELL 2: DATA LOADING ---
def load_and_prep():
    # Load Node Names
    try:
        ndf = pd.read_csv('/content/author_nodes_csLG_v1_2024_2025.csv')
        id_map = dict(zip(ndf['Id'], ndf['Label']))
    except: id_map = {}

    # Load Edges & Time Split
    df = pd.read_csv('/content/temporal_edges_csLG_v1_2024_2025.csv')
    df['V1_Date'] = pd.to_datetime(df['V1_Date'], dayfirst=True, errors='coerce')
    df = df.dropna(subset=['V1_Date']).sort_values('V1_Date')

    # Split 80-20
    split = int(len(df)*0.8)
    train_df = df.iloc[:split]
    test_df = df.iloc[split:]

    # Build Graph
    G_train = nx.Graph()
    for _, r in train_df.iterrows(): G_train.add_edge(r['Source_Id'], r['Target_Id'])

    print(f"Graph Nodes: {G_train.number_of_nodes()}, Edges: {G_train.number_of_edges()}")
    return train_df, test_df, G_train, id_map

def get_test_samples(G, test_df, n=2000):
    tr_nodes = set(G.nodes())
    pos, neg = [], []

    # Positive (Future edges)
    for _, r in test_df.iterrows():
        u, v = r['Source_Id'], r['Target_Id']
        if u in tr_nodes and v in tr_nodes and not G.has_edge(u, v):
            pos.append((u, v))
    if len(pos) > n: pos = random.sample(pos, n)

    # Negative
    all_n = list(tr_nodes)
    while len(neg) < len(pos):
        u, v = random.choice(all_n), random.choice(all_n)
        if u!=v and not G.has_edge(u, v) and (u,v) not in pos:
            neg.append((u, v))

    return pos + neg, [1]*len(pos) + [0]*len(neg)

train_df, test_df, G_train, id_map = load_and_prep()
samples, labels = get_test_samples(G_train, test_df)

# @title 3. ƒê·ªãnh nghƒ©a Model Core (ƒê√£ thay PageRank -> Personalized PageRank)
# --- CELL 3: MODELS ---
import networkx as nx
import numpy as np
import torch
import torch.nn as nn
from sklearn.decomposition import TruncatedSVD
from scipy import sparse
from networkx.algorithms.community import greedy_modularity_communities

# 1. HEURISTICS (ƒê√£ th√™m PPR)
class Heuristics:
    def __init__(self, G): self.G = G

    def cn(self, u, v): return len(list(nx.common_neighbors(self.G, u, v)))

    def aa(self, u, v):
        try: return next(nx.adamic_adar_index(self.G, [(u, v)]))[2]
        except: return 0

    def jc(self, u, v):
        try: return next(nx.jaccard_coefficient(self.G, [(u, v)]))[2]
        except: return 0

    def pa(self, u, v): return self.G.degree(u) * self.G.degree(v)

    def ra(self, u, v):
        return sum([1/self.G.degree(w) for w in nx.common_neighbors(self.G, u, v)])

    def katz(self, u, v, beta=0.005): return (beta**2) * self.cn(u, v)

    def hitting_time(self, u, v): return -1 * (self.G.degree(u) + self.G.degree(v)) # Approx

    # --- THAY ƒê·ªîI: PERSONALIZED PAGERANK ---
    def ppr(self, u, v):
        """
        T√≠nh Personalized PageRank t·ª´ u ƒë·∫øn v.
        L∆∞u √Ω: H√†m n√†y kh√° ch·∫≠m n·∫øu ch·∫°y cho h√†ng ngh√¨n c·∫∑p trong Cell 4.
        Ta gi·∫£m max_iter xu·ªëng 30 ƒë·ªÉ c√¢n b·∫±ng t·ªëc ƒë·ªô/ch√≠nh x√°c.
        """
        try:
            # personalization={u: 1} nghƒ©a l√† lu√¥n restart t·∫°i u
            # ƒêi·ªÅu n√†y ƒëo l∆∞·ªùng m·ª©c ƒë·ªô li√™n quan c·ªßa v ƒê·ªêI V·ªöI RI√äNG u
            scores = nx.pagerank(self.G, personalization={u: 1}, alpha=0.85, max_iter=30, tol=1e-4)
            return scores.get(v, 0)
        except: return 0

# 2. EMBEDDINGS (DeepWalk/Spectral & GraphSAGE)
def get_spectral_emb(G, dim=32):
    n = list(G.nodes())
    idx = {node: i for i, node in enumerate(n)}
    if len(n) == 0: return idx, np.array([])
    A = nx.to_scipy_sparse_array(G, nodelist=n)
    try:
        emb = TruncatedSVD(n_components=dim, random_state=42).fit_transform(A)
    except: # Fallback n·∫øu ƒë·ªì th·ªã qu√° nh·ªè
        emb = np.zeros((len(n), dim))
    return idx, emb

def get_graphsage_emb(G, base_emb, layers=2):
    n = list(G.nodes())
    if len(n) == 0: return base_emb
    A = nx.to_scipy_sparse_array(G, nodelist=n) + sparse.eye(len(n))
    # Normalize Adj: D^-1 * A
    degrees = np.array(A.sum(axis=1)).flatten()
    degrees[degrees==0] = 1
    D_inv = sparse.diags(1.0 / degrees)
    A_norm = D_inv.dot(A)

    curr = base_emb
    for _ in range(layers): curr = A_norm.dot(curr)
    return curr

# 3. SBM
def get_sbm_prob(G):
    try: comms = list(greedy_modularity_communities(G))
    except: comms = [list(G.nodes())]
    node_blk = {n: i for i, c in enumerate(comms) for n in c}
    n_blk = len(comms)

    counts = np.zeros((n_blk, n_blk))
    for u, v in G.edges():
        if u in node_blk and v in node_blk:
            counts[node_blk[u], node_blk[v]] += 1
            if node_blk[u] != node_blk[v]: counts[node_blk[v], node_blk[u]] += 1

    probs = np.zeros((n_blk, n_blk))
    sizes = [len(c) for c in comms]
    for i in range(n_blk):
        for j in range(n_blk):
            poss = sizes[i]*sizes[j] if i!=j else sizes[i]*(sizes[i]-1)/2
            if poss>0: probs[i, j] = counts[i, j]/poss
    return node_blk, probs

# 4. TGN (Simple)
class TGN(nn.Module):
    def __init__(self, n, d=32):
        super().__init__()
        self.mem = nn.Parameter(torch.randn(n, d))
        self.gru = nn.GRUCell(d, d)
        self.out = nn.Sequential(nn.Linear(d*2, 1), nn.Sigmoid())
    def forward(self, s, d): return self.out(torch.cat([self.mem[s], self.mem[d]], 1))
    def update(self, idx, val): self.mem.data[idx] = self.gru(val, self.mem[idx])

def train_tgn(train_df, G):
    nodes = list(G.nodes())
    imap = {n: i for i, n in enumerate(nodes)}
    model = TGN(len(nodes))
    opt = torch.optim.Adam(model.parameters(), lr=0.01)

    # L·∫•y danh s√°ch c·∫°nh c√≥ ID h·ª£p l·ªá
    data = []
    for _, r in train_df.iterrows():
        if r['Source_Id'] in imap and r['Target_Id'] in imap:
            data.append((imap[r['Source_Id']], imap[r['Target_Id']]))

    if not data: return model, imap

    for _ in range(2): # Epochs
        random.shuffle(data)
        # Train batch nh·ªè ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian
        batch_size = min(1000, len(data))
        batch = data[:batch_size]
        s = torch.tensor([x[0] for x in batch])
        d = torch.tensor([x[1] for x in batch])

        preds = model(s, d).squeeze()
        loss = nn.BCELoss()(preds, torch.ones(len(batch)))
        opt.zero_grad(); loss.backward(); opt.step()

        with torch.no_grad():
            model.update(s, model.mem[d]); model.update(d, model.mem[s])
    return model, imap

# @title 4. ƒê√°nh gi√° & Ch·ªçn Top 3 M√¥ h√¨nh (ƒê√£ s·ª≠a l·ªói & T·ªëi ∆∞u)
# --- CELL 4: EVAL & OPTIMIZE TOP 3 ---
import pandas as pd
import numpy as np
import networkx as nx
from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_curve
from sklearn.metrics.pairwise import cosine_similarity

print("üöÄ ƒêang ch·∫°y ƒë√°nh gi√° to√†n b·ªô m√¥ h√¨nh...")

# 1. Hu·∫•n luy·ªán s∆° b·ªô (Pre-training) & Chu·∫©n b·ªã d·ªØ li·ªáu
# ƒê·∫£m b·∫£o c√°c bi·∫øn t·ª´ Cell 2 & 3 ƒë√£ s·∫µn s√†ng
try:
    # Heuristics
    h = Heuristics(G_train)

    # PageRank (Global) - D√πng ƒë·ªÉ ƒë√°nh gi√° nhanh
    pr = nx.pagerank(G_train)

    # Embeddings
    n_idx, spec_emb = get_spectral_emb(G_train)
    # N·∫øu ch∆∞a c√≥ sage_emb th√¨ ch·∫°y l·∫°i h√†m
    sage_emb = get_graphsage_emb(G_train, spec_emb)

    # SBM
    n_blk, sbm_prob = get_sbm_prob(G_train)

    # TGN (Ki·ªÉm tra xem ƒë√£ train ch∆∞a, n·∫øu ch∆∞a th√¨ train nhanh)
    if 'tgn' not in globals():
        print("   -> ƒêang hu·∫•n luy·ªán TGN...")
        tgn, t_map = train_tgn(train_df, G_train)
    t_mem = tgn.mem.detach().cpu().numpy() # ƒê·∫£m b·∫£o chuy·ªÉn v·ªÅ CPU/Numpy
except Exception as e:
    print(f"‚ö†Ô∏è C√≥ l·ªói trong qu√° tr√¨nh chu·∫©n b·ªã m√¥ h√¨nh: {e}")

# 2. Ch·∫•m ƒëi·ªÉm t·∫≠p test (Scoring)
print("   -> ƒêang t√≠nh ƒëi·ªÉm cho t·∫≠p Test...")
scores = {k: [] for k in ['CN','AA','JC','PA','RA','Katz','HitTime','PageRank','DeepWalk','GraphSAGE','SBM','TGN']}

for u, v in samples:
    # --- A. Heuristics ---
    scores['CN'].append(h.cn(u, v))
    scores['AA'].append(h.aa(u, v))
    scores['JC'].append(h.jc(u, v))
    scores['PA'].append(h.pa(u, v))
    scores['RA'].append(h.ra(u, v))
    scores['Katz'].append(h.katz(u, v))
    scores['HitTime'].append(h.hitting_time(u, v))
    scores['PageRank'].append(pr.get(u,0) * pr.get(v,0))

    # --- B. Embeddings ---
    if u in n_idx and v in n_idx:
        # DeepWalk (Spectral)
        scores['DeepWalk'].append(cosine_similarity([spec_emb[n_idx[u]]], [spec_emb[n_idx[v]]])[0][0])
        # GraphSAGE
        scores['GraphSAGE'].append(cosine_similarity([sage_emb[n_idx[u]]], [sage_emb[n_idx[v]]])[0][0])
    else:
        scores['DeepWalk'].append(0); scores['GraphSAGE'].append(0)

    # --- C. SBM ---
    if u in n_blk and v in n_blk:
        scores['SBM'].append(sbm_prob[n_blk[u], n_blk[v]])
    else:
        scores['SBM'].append(0)

    # --- D. TGN ---
    if u in t_map and v in t_map:
        scores['TGN'].append(cosine_similarity([t_mem[t_map[u]]], [t_mem[t_map[v]]])[0][0])
    else:
        scores['TGN'].append(0)

# 3. Ch·ªçn Top 3 M√¥ h√¨nh d·ª±a tr√™n AUC
metrics_list = []
for name, s in scores.items():
    try:
        # X·ª≠ l√Ω NaN/Inf n·∫øu c√≥
        s_clean = np.nan_to_num(s)
        auc = roc_auc_score(labels, s_clean)
        metrics_list.append({'Model': name, 'AUC': auc, 'Raw_Scores': s_clean})
    except Exception as e:
        print(f"   L·ªói t√≠nh AUC cho {name}: {e}")

# S·∫Øp x·∫øp gi·∫£m d·∫ßn theo AUC v√† l·∫•y top 3
sorted_metrics = sorted(metrics_list, key=lambda x: x['AUC'], reverse=True)
top_3_models = sorted_metrics[:3]

print(f"\nüèÜ TOP 3 M√î H√åNH T·ªêT NH·∫§T:")
for rank, m in enumerate(top_3_models, 1):
    print(f"{rank}. {m['Model']} (AUC={m['AUC']:.4f})")

# 4. T·ªëi ∆∞u h√≥a Metrics (Acc, F1) cho Top 3
top_3_optimized = []

print(f"\n‚öôÔ∏è ƒêang t·ªëi ∆∞u h√≥a ng∆∞·ª°ng (Threshold) cho Top 3...")
for m in top_3_models:
    name = m['Model']
    raw_s = m['Raw_Scores']
    auc = m['AUC']

    # T√¨m ng∆∞·ª°ng t·ªëi ∆∞u F1
    precision, recall, thresholds = precision_recall_curve(labels, raw_s)

    # T√≠nh F1 (x·ª≠ l√Ω chia cho 0)
    numerator = 2 * precision * recall
    denominator = precision + recall
    f1_scores = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)

    # L·∫•y v·ªã tr√≠ F1 cao nh·∫•t
    best_idx = np.argmax(f1_scores)

    # L∆∞u √Ω: thresholds ng·∫Øn h∆°n precision/recall 1 ph·∫ßn t·ª≠
    if best_idx < len(thresholds):
        opt_thresh = thresholds[best_idx]
        opt_f1 = f1_scores[best_idx]
    else:
        opt_thresh = 0.5 # Fallback
        opt_f1 = 0.0

    # T√≠nh Accuracy t·∫°i ng∆∞·ª°ng ƒë√≥
    y_pred = [1 if s >= opt_thresh else 0 for s in raw_s]
    opt_acc = accuracy_score(labels, y_pred)

    # L∆∞u k·∫øt qu·∫£
    model_meta = {
        'model_name': name,
        'threshold': opt_thresh,
        'metrics': {'AUC': auc, 'Accuracy': opt_acc, 'F1-Score': opt_f1}
    }
    top_3_optimized.append(model_meta)

# Hi·ªÉn th·ªã b·∫£ng k·∫øt qu·∫£ cu·ªëi c√πng
print("\nüìä B·∫¢NG K·∫æT QU·∫¢ ƒê√É T·ªêI ∆ØU CHO TOP 3:")
results_df = pd.DataFrame([
    {
        'Model': x['model_name'],
        'AUC': x['metrics']['AUC'],
        'Optimized Accuracy': x['metrics']['Accuracy'],
        'Optimized F1': x['metrics']['F1-Score'],
        'Best Threshold': x['threshold']
    } for x in top_3_optimized
])
display(results_df)

# @title 5. ƒê√≥ng g√≥i Tr·∫°ng th√°i Top 3 Models
# --- CELL 5: SAVE TOP 3 PACKAGE ---
import pickle

# T·∫°o g√≥i d·ªØ li·ªáu (Package)
# Ch√∫ng ta c·∫ßn l∆∞u data c·ªßa ALL models ti·ªÅm nƒÉng ƒë·ªÉ ƒë·∫£m b·∫£o
# b·∫•t k·ªÉ Top 3 l√† ai (SBM hay GraphSAGE...), ta ƒë·ªÅu c√≥ d·ªØ li·ªáu ƒë·ªÉ ch·∫°y n√≥.
full_package = {
    'top_3_meta': top_3_optimized, # Ch·ª©a t√™n model v√† ng∆∞·ª°ng threshold
    'graph': G_train,
    'id_map': id_map,

    # D·ªØ li·ªáu ph·ª•c v·ª• t√≠nh to√°n (L∆∞u h·∫øt ƒë·ªÉ d·ª± ph√≤ng)
    'data_store': {
        'sbm': {'node_block': n_blk, 'probs': sbm_prob},
        'emb': {'idx': n_idx, 'spec': spec_emb, 'sage': sage_emb},
        'tgn': {'map': t_map, 'mem': t_mem},
        'pagerank': pr
    }
}

filename = 'top3_models_optimized.pkl'
with open(filename, 'wb') as f:
    pickle.dump(full_package, f)

print(f"üíæ ƒê√£ l∆∞u g√≥i m√¥ h√¨nh v√†o '{filename}'")
print(f"üëâ G·ªìm c√°c m√¥ h√¨nh: {', '.join([m['model_name'] for m in top_3_optimized])}")

# @title 6. Giao di·ªán D·ª± ƒëo√°n (Top 3 - C√≥ PPR Chu·∫©n)
# --- CELL 6: PREDICT WITH REAL PPR ---
import pickle
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity

def recommend_with_top3(author_id_input):
    try:
        with open('top3_models_optimized.pkl', 'rb') as f:
            pkg = pickle.load(f)
    except:
        print("‚ö†Ô∏è Ch∆∞a t√¨m th·∫•y file model. H√£y ch·∫°y Cell 5.")
        return

    G = pkg['graph']
    top3 = pkg['top_3_meta']
    names = pkg['id_map']
    store = pkg['data_store']

    # X·ª≠ l√Ω ID
    try: author_id = int(author_id_input)
    except: author_id = str(author_id_input)

    if author_id not in G:
        alt = str(author_id) if isinstance(author_id, int) else int(author_id)
        if alt in G: author_id = alt
        else:
            print(f"‚ùå ID '{author_id_input}' kh√¥ng t·ªìn t·∫°i.")
            return

    u_name = names.get(author_id, str(author_id))
    print(f"\nüîç G·ª¢I √ù H·ª¢P T√ÅC CHO: {u_name} (ID: {author_id})")

    neighbors = set(G.neighbors(author_id))
    all_nodes = list(G.nodes())

    # --- T√çNH TO√ÅN PPR VECTOR RI√äNG CHO USER N√ÄY ---
    # ƒê·ªÉ ƒë·∫£m b·∫£o t·ªëc ƒë·ªô, ta ch·ªâ t√≠nh 1 l·∫ßn n·∫øu trong Top 3 c√≥ PageRank
    ppr_vector = {}
    has_pagerank_in_top3 = any(m['model_name'] == 'PageRank' for m in top3)

    if has_pagerank_in_top3:
        print("   ‚ö° ƒêang t√≠nh Personalized PageRank (C√° nh√¢n h√≥a)...")
        # Personalization key: author_id = 1
        ppr_vector = nx.pagerank(G, personalization={author_id: 1}, alpha=0.85)

    # V√≤ng l·∫∑p Top 3
    for rank, model_info in enumerate(top3, 1):
        m_name = model_info['model_name']
        m_auc = model_info['metrics']['AUC']

        print(f"\nüîπ {rank}. Theo {m_name} (AUC: {m_auc:.3f}):")

        candidates = []

        # Load data
        sbm = store['sbm']
        emb = store['emb']
        tgn = store['tgn']
        # Pr (Global) kh√¥ng d√πng n·ªØa n·∫øu model l√† PageRank, v√¨ ta d√πng PPR

        for target in all_nodes:
            if target == author_id or target in neighbors: continue

            score = -1

            # === LOGIC T√çNH ƒêI·ªÇM ===

            # 1. PPR (Thay th·∫ø cho Global PageRank)
            if m_name == 'PageRank':
                score = ppr_vector.get(target, 0)

            # 2. Heuristics
            elif m_name == 'CN': score = len(list(nx.common_neighbors(G, author_id, target)))
            elif m_name == 'AA':
                try: score = next(nx.adamic_adar_index(G, [(author_id, target)]))[2]
                except: score = 0
            elif m_name == 'JC':
                try: score = next(nx.jaccard_coefficient(G, [(author_id, target)]))[2]
                except: score = 0
            elif m_name == 'PA': score = G.degree(author_id) * G.degree(target)
            elif m_name == 'RA': score = sum([1/G.degree(w) for w in nx.common_neighbors(G, author_id, target)])
            elif m_name == 'Katz': score = len(list(nx.common_neighbors(G, author_id, target)))
            elif m_name == 'HitTime': score = -1*(G.degree(author_id) + G.degree(target))

            # 3. SBM
            elif m_name == 'SBM':
                nb, bp = sbm['node_block'], sbm['probs']
                if author_id in nb and target in nb:
                    score = bp[nb[author_id], nb[target]]

            # 4. Embeddings
            elif m_name in ['GraphSAGE', 'DeepWalk']:
                idx = emb['idx']
                vec = emb['sage'] if m_name=='GraphSAGE' else emb['spec']
                if author_id in idx and target in idx:
                    v1 = vec[idx[author_id]].reshape(1, -1)
                    v2 = vec[idx[target]].reshape(1, -1)
                    score = cosine_similarity(v1, v2)[0][0]

            # 5. TGN
            elif m_name == 'TGN':
                tm, tmem = tgn['map'], tgn['mem']
                if author_id in tm and target in tm:
                    v1 = tmem[tm[author_id]].reshape(1, -1)
                    v2 = tmem[tm[target]].reshape(1, -1)
                    score = cosine_similarity(v1, v2)[0][0]

            if score > -99:
                candidates.append((target, score, G.degree(target)))

        # S·∫Øp x·∫øp
        candidates.sort(key=lambda x: (x[1], x[2]), reverse=True)

        # In Top 10
        print(f"   {'T√™n':<25} | {'ID':<8} | {'Score':<8} | {'Deg':<4}")
        print("   " + "-"*50)

        for uid, sc, dg in candidates[:10]:
            n_str = names.get(uid, str(uid))
            n_str = (str(n_str)[:23] + '..') if len(str(n_str))>25 else str(n_str)
            print(f"   {n_str:<25} | {str(uid):<8} | {sc:.4f}   | {dg:<4}")

# --- Loop ---
while True:
    i = input("\n>> Nh·∫≠p ID (exit): ")
    if i.lower() == 'exit': break
    try: recommend_with_top3(i)
    except Exception as e: print(f"L·ªói: {e}")